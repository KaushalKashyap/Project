# -*- coding: utf-8 -*-
"""Textual analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qbVwo1cov7qSdgRecKgEsEOiuV6OGHSS
"""

# Necessary Libraries
import pandas as pd
import requests
import bs4 as BeautifulSoup
import nltk
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import string
from textblob import TextBlob
import csv
import numpy as np

information= pd.read_excel('/content/Input.xlsx')

information.head(10)

# # Extraction the data
# for i, url in enumerate(information['URL']):
#     # Send a request to the URL and get the HTML content
#     response = requests.get(url)
#     html_content = response.text

#     # Parse the HTML content using BeautifulSoup
#     soup = BeautifulSoup(html_content, 'html.parser')

#     # Extract the title
#     title = soup.find('title').text

#     # Extract the article
#     article = '\n'.join([p.text for p in soup.find_all('p')])

# # Save the title and article in separate text files
# with open(f'extracted.txt', 'w',encoding='utf-8') as f:
#     f.write(title + '\n\n' + article)

# Extraction of the data
title=[]
article=[]
for i, url in enumerate(information['URL']):
    # Send a request to the URL and get the HTML content
    response = requests.get(url)
    html_content = response.text

    # Parse the HTML content using BeautifulSoup
    soup = BeautifulSoup.BeautifulSoup(html_content, 'html.parser')

    # Extract the title
    title.append(soup.find('title').text)

    # Extract the article
    article.append([p.text for p in soup.find_all('p')])

# # Save the title and article in separate text files
# with open(f'extracted.txt', 'w',encoding='utf-8') as f:
#     f.write(title + '\n\n' + article)

information['title']= title

article

# Clearing the symbols between fetched articles
for i in range(len(article)):
  article[i]=[z.replace('?','').replace('.','').replace(',','').replace('!','').replace('\n','').replace('\t','').replace('"','').replace('(','').replace(')','').replace("'",'') for z in article[i]]

nltk.download('stopwords')

stop_words= list(set(stopwords.words('english')))

nltk.download('punkt')

# Sentence Tokenization
sentencess = []
for z in article:
  for art in z:
    sentencess.append((sent_tokenize(art)))

sentencess

# Count of sentence in the article
sentences = []
for z in article:
  count=0
  for art in z:
    count+= len(sent_tokenize(art))
  sentences.append(count)

sentences

# cleaned data for complex word and syllables calculation
cleaned_articles=[]
for i in range(len(article)):
  for w in stop_words:
    a= [z.replace(' '+w+' ',' ').replace('?','').replace('.','').replace(',','').replace('!','').replace('%','').replace('-','').replace('$','').replace('*','') for z in article[i]]
  # cleaned_articles[i].append(a)
  cleaned_articles+=[a]

len(cleaned_articles)

cleaned_articles

# Word Tokenization
words = []
for art in article:
  count=0
  for a in art:
    count=(len(word_tokenize(a)))
  words.append(count)

# Word Count
words

# no use but for evaluation purpose .. word tokenization
words_cleaned = []
for art in article:
  # for a in art:
    words_cleaned.append(len(word_tokenize(a)))

# Positive word dictionary loading here
positive_words=pd.read_csv('/content/positive-words.txt',encoding='latin-1')

print(type(positive_words))

positive_words

# Negative word dictionary loading here
negative_words=pd.read_csv('/content/negative-words.txt',sep='\t',header=None,encoding='latin-1')

# Dataframing and Renaming here
negative_wordss=pd.DataFrame(negative_words)

negative_wordss.set_axis(['neg'], axis='columns', inplace=True)

negative_wordss

# for Lowercase to text
cleaned_articlesd=[]
for i in cleaned_articles:
  for j in i:
    a = j.lower()
  cleaned_articlesd+=[a]

len(cleaned_articlesd)

#  for split to word level the string..
clean_articled=[]
for i in cleaned_articlesd:
  clean_articled.append(i.split(' '))

len(clean_articled)

# Calculating Positive Score
positive_score=[]
for letter in clean_articled:
  count=0
  for word in positive_words['a+']:
    for l in letter:
      if word == l:
        count+=1
  positive_score.append(count)

len(positive_score)

positive_score

# Same Negative score calculation
negative_score=[]
for letter in clean_articled:
  count=0
  for word in negative_wordss['neg']:
    for l in letter:
      if word == l:
        count+=1
  negative_score.append(count)

negative_score

words_cleaned = np.array(words_cleaned)
sentences = np.array(sentences)

# framing in information dataframe
information['POSITIVE SCORE']= pd.Series(positive_score)
information['NEGATIVE SCORE']= pd.Series(negative_score)

# Polarity Score
information['POLARITY SCORE'] = (information['POSITIVE SCORE']-information['NEGATIVE SCORE'])/ ((information['POSITIVE SCORE'] +information['NEGATIVE SCORE']) + 0.000001)

information['SUBJECTIVITY SCORE'] = (information['POSITIVE SCORE'] + information['NEGATIVE SCORE'])/( (words_cleaned) + 0.000001)

information['AVG SENTENCE LENGTH'] = np.array(words)/np.array(sentences)

complex_words = []
sylabble_counts = []

for art in article:
  sylabble_count=0
  d=clean_articled
  ans=0
  for word in d:
    count=0
    for w in word:
      for i in range(len(w)):
        if(w[i]=='a' or w[i]=='e' or w[i] =='i' or w[i] == 'o' or w[i] == 'u'):
          count+=1
        if(i==len(w)-2 and (w[i]=='e' and w[i+1]=='d')):
          count-=1;
        if(i==len(w)-2 and (w[i]=='e' and w[i]=='s')):
          count-=1;
  sylabble_count+=count    
  if(count>2):
    ans+=1
  sylabble_counts.append(sylabble_count)
  complex_words.append(ans)

information['PERCENTAGE OF COMPLEX WORDS'] = np.array(complex_words)/np.array(words)

information['FOG INDEX'] = 0.4 * (information['AVG SENTENCE LENGTH'] + information['PERCENTAGE OF COMPLEX WORDS'])

information['AVG NUMBER OF WORDS PER SENTENCES'] = information['AVG SENTENCE LENGTH']

information['COMPLEX WORD COUNT'] = complex_words

information['WORD COUNT'] = words

information['SYLLABLE PER WORD'] = np.array(sylabble_counts)/np.array(words)

total_characters = []
for art in article:
  characters = 0
  for word in art:
    characters+=len(word)
  total_characters.append(characters)

total_characters

personal_nouns = []
personal_noun =['I', 'we','my', 'ours','and' 'us','My','We','Ours','Us','And'] 
for art in article:
  ans=0
  for word in art:
    for w in word:
      for p in personal_noun:
        if p == w:
          ans=ans+1
  personal_nouns.append(ans)

personal_nouns

information['PERSONAL PRONOUN'] = personal_nouns

information['AVG WORD LENGTH'] = np.array(total_characters)/np.array(words)

information

information.to_excel("Output Data Structure.xlsx")

article

